\documentclass[10pt, pdf, hyperref={unicode}]{beamer}

%%% НАСТРОЙКИ ЯЗЫКА %%%
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}

%%% ОФОРМЛЕНИЕ ТЕМЫ %%%
\usetheme{Madrid}
\usecolortheme{whale}

% Убираем навигационные значки
\setbeamertemplate{navigation symbols}{}

%%% ЛОГОТИП %%%
% Убедись, что файл bsu_logo.png лежит в папке images
\logo{\includegraphics[width=2.5cm]{bsu_logo.png}} 

%%% МАТЕМАТИКА И КАРТИНКИ %%%
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\graphicspath{{images/}} 

%%% ИНФОРМАЦИЯ О ПРЕЗЕНТАЦИИ %%%
\title[EM-алгоритм и GMM]{Исследование и практическая реализация EM-алгоритма для задачи кластеризации (GMM)}
\subtitle{Курсовой проект}
\author[Санчук С. А.]{Студент: Санчук Сергей Александрович\\ \small{Руководитель: Буславский Александр Андреевич}}
\institute[БГУ]{Белорусский государственный университет\\ФПМИ, Кафедра ДМА}
\date{Минск, 2025}

\begin{document}

% 1. Титульный слайд
\begin{frame}
    \titlepage
\end{frame}

% 2. Введение
\begin{frame}{Введение}
    \textbf{Цель работы:} Теоретическое исследование EM-алгоритма и разработка программной реализации модели GMM для кластеризации данных сложной структуры.

    \vspace{0.5cm}
    \textbf{Основные задачи:}
    \begin{itemize}
        \item Провести теоретический анализ модели смеси гауссовых распределений (GMM).
        \item Вывести формулы EM-алгоритма через вариационную нижнюю границу (ELBO).
        \item Реализовать алгоритм на Python с поддержкой полной ковариационной матрицы.
        \item Сравнить эффективность GMM и K-means на синтетических и реальных данных.
    \end{itemize}
\end{frame}

% 3. Модель GMM
\begin{frame}{Математическая модель GMM}
    Плотность вероятности наблюдаемой переменной $x$:
    \begin{block}{Формула смеси}
        $$ p(x|\theta) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x | \mu_k, \Sigma_k) $$
    \end{block}
    
    \begin{columns}
        \column{0.5\textwidth}
        Параметры $\theta$:
        \begin{itemize}
            \item $\pi_k$ — вес компоненты ($\sum \pi_k = 1$);
            \item $\mu_k$ — центр кластера;
            \item $\Sigma_k$ — ковариационная матрица.
        \end{itemize}
        
        \column{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{img_01.png}
    \end{columns}
\end{frame}

% 5. MLE проблема
\begin{frame}{Метод максимального правдоподобия (MLE)}
    Необходимо максимизировать логарифм функции правдоподобия:
    
    $$ \ln p(X|\theta) = \sum_{n=1}^{N} \ln \left( \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k) \right) \to \max_{\theta} $$
    
    \vspace{0.5cm}
    \textbf{Проблема:}
    Сумма находится под знаком логарифма ($\ln \sum$).
    \begin{itemize}
        \item Аналитическое решение невозможно.
        \item Необходим итерационный метод (EM-алгоритм).
    \end{itemize}
\end{frame}

% 5. Нижняя граница (ELBO) - Объединенный
\begin{frame}{Вариационная нижняя граница (ELBO)}
    Для формализации задачи вводятся латентные переменные $Z$ и вариационное распределение $q(Z)$.
    
    Используя декомпозицию правдоподобия и неотрицательность $KL$-дивергенции ($KL \ge 0$), получаем нижнюю оценку:
    
    $$ \ln p(X|\theta) = \mathcal{L}(q, \theta) + KL(q(Z|X) || p(Z|X, \theta)) \ge \mathcal{L}(q, \theta) $$
    
    Раскроем структуру ELBO (как разность Энергии и Энтропии):
    \begin{block}{Структура ELBO}
        $$ \mathcal{L}(q, \theta) = \underbrace{\mathbb{E}_q [\ln p(X, Z | \theta)]}_{\text{Energy (Ожидаемое правдоподобие)}} - \underbrace{\mathbb{E}_q [\ln q(Z)]}_{\text{Entropy (Энтропия)}} $$
    \end{block}
    
    \textbf{Суть EM-алгоритма:} Мы итеративно максимизируем этот функционал, приближая нижнюю границу к истинному правдоподобию.
\end{frame}

% 7. Общая схема алгоритма (ArgMax + ArgMin KL) - Исправлено
\begin{frame}{Общая схема итерации EM}
    Алгоритм состоит из попеременной оптимизации границы $\mathcal{L}(q, \theta)$:
    
    \begin{block}{E-step (Optimization w.r.t. $q$)}
        Фиксируем $\theta^{(t)}$. Находим распределение $q$, максимизирующее ELBO (что эквивалентно минимизации KL-дивергенции):
        $$ q^{(t+1)} = \arg\max_{q} \mathcal{L}(q, \theta^{(t)}) = \arg\min_{q} KL(q || p) $$
        Решение: $q^{(t+1)}(Z) = p(Z | X, \theta^{(t)})$ (апостериорное распределение).
    \end{block}
    
    \begin{block}{M-step (Optimization w.r.t. $\theta$)}
        Фиксируем $q^{(t+1)}$. Находим параметры $\theta$, максимизирующие Энергию:
        $$ \theta^{(t+1)} = \arg\max_{\theta} \mathbb{E}_{q^{(t+1)}} [\ln p(X, Z | \theta)] $$
    \end{block}
\end{frame}

% 7. Скрытые переменные в GMM (Обновленный)
\begin{frame}{Скрытые переменные в задаче GMM}
    Что выбрать в качестве скрытых переменных?
    \begin{itemize}
        \item $z_{nk} \in \{0, 1\}$ — индикатор: принадлежит ли объект $x_n$ компоненте $k$.
        \item $\gamma_{nk} = p(z_{nk}=1|x_n)$ — вероятность принадлежности.
    \end{itemize}
    
    \begin{enumerate}
        \item На E-шаге нужно минимизировать $KL(q(Z|X) || p(Z|X, \theta))$.
        \item Минимум $KL = 0$ достигается, когда $q(Z|X) = p(Z|X, \theta)$.
    \end{enumerate}
    
    По формуле Байеса истинное апостериорное распределение:
    $$ p(z_{nk}=1|x_nб \mu_k, \Sigma_k) = \frac{\pi_k \mathcal{N}(x_n|\mu_k, \Sigma_k)}{\sum_{j} \pi_j \mathcal{N}(x_n|\mu_j, \Sigma_j)} = \gamma_{nk} $$
    
    \textbf{Вывод:} Полагая $q(z_{nk}=1) = \gamma_{nk}$, мы делаем $KL=0$ и $ELBO = \ln p(X)$.
\end{frame}

% 9. M-шаг для GMM (Детализация)
\begin{frame}{M-шаг для смеси гауссиан}
    Максимизируем ожидаемое правдоподобие:
    $$ Q(\theta) = \sum_{n=1}^{N} \sum_{k=1}^{K} \gamma_{nk} \ln \left( \pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k) \right) \to \max_{\pi, \mu, \Sigma} $$
    
    \textbf{Результаты оптимизации:}
    \begin{enumerate}
        \item \textbf{Веса $\pi_k$:} Используя метод множителей Лагранжа (ограничение $\sum \pi_k = 1$), получаем:
        $$ \pi_k^{new} = \frac{1}{N} \sum_{n=1}^{N} \gamma_{nk} $$
        
        \item \textbf{Параметры Гауссиан ($\mu_k, \Sigma_k$):} Приравнивая производные к нулю:
        $$ \mu_k^{new} = \frac{\sum_n \gamma_{nk} x_n}{\sum_n \gamma_{nk}}, \quad \Sigma_k^{new} = \frac{\sum_n \gamma_{nk} (x_n - \mu_k)(x_n - \mu_k)^T}{\sum_n \gamma_{nk}} $$
    \end{enumerate}
\end{frame}

% 11. Связь с K-means (Обновленный)
\begin{frame}{Связь с алгоритмом K-means}
    Рассмотрим GMM с ограничениями: $\Sigma_k = \epsilon I, \quad \pi_k = 1/K$.
    
    Подставим плотность нормального распределения в формулу $\gamma_{nk}$:
    $$ \gamma_{nk} = \frac{ \exp \left( - \frac{1}{2\epsilon} \|x_n - \mu_k\|^2 \right) }{ \sum_{j=1}^K \exp \left( - \frac{1}{2\epsilon} \|x_n - \mu_j\|^2 \right) } $$
    
    \textbf{Предельный переход $\epsilon \to 0$:}
    В сумме доминирует слагаемое с минимальным расстоянием $\|x_n - \mu\|^2$.
    $$ \lim_{\epsilon \to 0} \gamma_{nk} = \begin{cases} 1, & \text{если } k = \arg\min_j \|x_n - \mu_j\|^2 \\ 0, & \text{иначе} \end{cases} $$
    
    Мягкая вероятность становится жесткой меткой (Hard assignment).
    
    \vspace{0.1cm}
    \begin{center}
        \includegraphics[height=0.35\textheight, keepaspectratio]{img_05.png}
    \end{center}
\end{frame}

% 12. Эксперимент: Сходимость
\begin{frame}{Эксперимент 1: Анализ сходимости}
    На графике представлена зависимость Log-Likelihood от номера итерации. Алгоритм монотонно увеличивает правдоподобие.
    
    \begin{center}
        \includegraphics[width=0.7\textwidth]{img_03.png}
    \end{center}
    
    \small{Реализована гибридная инициализация (K-means + EM) для избежания локальных минимумов.}
\end{frame}

% 13. Эксперимент: Анизотропия
\begin{frame}{Эксперимент 2: Анизотропные данные}
    Сравнение на вытянутых кластерах.
    
    \begin{columns}
        \column{0.4\textwidth}
        \begin{itemize}
            \item \textbf{K-means:} «Разрезает» кластеры, так как ищет сферы.
            \item \textbf{GMM:} Обучает полную матрицу $\Sigma$, подстраиваясь под наклон данных.
        \end{itemize}
        
        \column{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img_05.png}
    \end{columns}
\end{frame}

% 14. Эксперимент: Разная плотность
\begin{frame}{Эксперимент 3: Различная дисперсия}
    Ситуация: плотный кластер рядом с разреженным.
    
    \begin{center}
        \includegraphics[width=0.75\textwidth]{img_06.png}
    \end{center}
    
    GMM корректно определяет границы, учитывая разный «размер» (дисперсию) кластеров.
\end{frame}

% 15. Эксперимент: Сложная форма
\begin{frame}{Эксперимент 4: Невыпуклые данные}
    Аппроксимация датасета «Moons» ($K=10$).
    
    \begin{center}
        \includegraphics[width=0.6\textwidth]{img_07.png}
    \end{center}
    
    GMM может использоваться как универсальный аппроксиматор произвольной плотности вероятности.
\end{frame}

% 16. Эксперимент: Реальные данные
\begin{frame}{Эксперимент 5: Ирисы Фишера}
    Визуализация кластеризации реальных данных.
    
    \begin{center}
        \includegraphics[width=0.65\textwidth]{img_08.png}
    \end{center}
    
    Для пересекающихся классов (\textit{Versicolor} и \textit{Virginica}) алгоритм возвращает мягкие вероятности ($\gamma \approx 0.5$).
\end{frame}

% 17. Заключение
\begin{frame}{Заключение}
    \textbf{Результаты работы:}
    \begin{enumerate}
        \item Изучен математический аппарат EM-алгоритма.
        \item Разработана эффективная реализация на Python (Vectorization, Log-Sum-Exp trick).
        \item Экспериментально подтверждено преимущество GMM перед K-means на данных сложной структуры (анизотропия, разная плотность).
    \end{enumerate}
    
    \vspace{1cm}
    \centering
    \Large \textbf{Спасибо за внимание!}
\end{frame}

\end{document}