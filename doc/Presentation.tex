\documentclass[10pt, pdf, hyperref={unicode}]{beamer}

%%% НАСТРОЙКИ ЯЗЫКА %%%
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}

%%% ОФОРМЛЕНИЕ ТЕМЫ %%%
\usetheme{Madrid}
\usecolortheme{whale}

% Убираем навигационные значки
\setbeamertemplate{navigation symbols}{}

%%% ЛОГОТИП %%%
% Убедись, что файл bsu_logo.png лежит в папке images
\logo{\includegraphics[width=2.5cm]{bsu_logo.png}} 

%%% МАТЕМАТИКА И КАРТИНКИ %%%
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\graphicspath{{images/}} 

%%% ИНФОРМАЦИЯ О ПРЕЗЕНТАЦИИ %%%
\title[EM-алгоритм и GMM]{Исследование и практическая реализация EM-алгоритма для задачи кластеризации (GMM)}
\subtitle{Курсовой проект}
\author[Санчук С. А.]{Студент: Санчук Сергей Александрович\\ \small{Руководитель: Буславский Александр Андреевич}}
\institute[БГУ]{Белорусский государственный университет\\ФПМИ, Кафедра ДМА}
\date{Минск, 2025}

\begin{document}

% 1. Титульный слайд
\begin{frame}
    \titlepage
\end{frame}

% 2. Введение
\begin{frame}{Введение}
    \textbf{Цель работы:} Теоретическое исследование EM-алгоритма и разработка программной реализации модели GMM для кластеризации данных сложной структуры.

    \vspace{0.5cm}
    \textbf{Основные задачи:}
    \begin{itemize}
        \item Провести теоретический анализ модели смеси гауссовых распределений (GMM).
        \item Вывести формулы EM-алгоритма через вариационную нижнюю границу (ELBO).
        \item Реализовать алгоритм на Python (NumPy) с поддержкой полной ковариационной матрицы.
        \item Сравнить эффективность GMM и K-means на синтетических и реальных данных.
    \end{itemize}
\end{frame}

% 3. Модель GMM
\begin{frame}{Математическая модель GMM}
    Плотность вероятности наблюдаемой переменной $x$:
    \begin{block}{Формула смеси}
        $$ p(x|\theta) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x | \mu_k, \Sigma_k) $$
    \end{block}
    
    \begin{columns}
        \column{0.5\textwidth}
        Параметры $\theta$:
        \begin{itemize}
            \item $\pi_k$ — вес компоненты ($\sum \pi_k = 1$);
            \item $\mu_k$ — центр кластера;
            \item $\Sigma_k$ — ковариационная матрица.
        \end{itemize}
        
        \column{0.5\textwidth}
        \centering
        \includegraphics[width=0.9\textwidth]{img_01.png}
    \end{columns}
\end{frame}

% 4. Скрытые переменные
\begin{frame}{Скрытые переменные}
    Для формализации задачи вводятся латентные переменные $Z$.
    
    \begin{itemize}
        \item Пусть $z$ — $K$-мерный бинарный вектор, указывающий, из какой компоненты взят объект.
        \item Априорное распределение: $p(z_k=1) = \pi_k$.
        \item Условное распределение: $p(x|z_k=1) = \mathcal{N}(x|\mu_k, \Sigma_k)$.
    \end{itemize}

    \vspace{0.5cm}
    \textbf{Байесовский вывод:}
    Нам неизвестны значения $Z$. Задача состоит в оценке апостериорной вероятности $p(Z|X)$.
\end{frame}

% 5. MLE проблема
\begin{frame}{Метод максимального правдоподобия (MLE)}
    Необходимо максимизировать логарифм функции правдоподобия:
    
    $$ \ln p(X|\theta) = \sum_{n=1}^{N} \ln \left( \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k) \right) \to \max_{\theta} $$
    
    \vspace{0.5cm}
    \textbf{Проблема:}
    Сумма находится под знаком логарифма ($\ln \sum$).
    \begin{itemize}
        \item Аналитическое решение невозможно.
        \item Необходим итерационный метод (EM-алгоритм).
    \end{itemize}
\end{frame}

% 6. Декомпозиция (Идея)
\begin{frame}{Декомпозиция правдоподобия}
    Вводим вариационное распределение $q(Z)$. Правдоподобие раскладывается на две составляющие:
    
    \begin{block}{Фундаментальное тождество}
        $$ \ln p(X|\theta) = \mathcal{L}(q, \theta) + KL(q || p) $$
    \end{block}
    
    Где:
    \begin{itemize}
        \item $\mathcal{L}(q, \theta)$ — Вариационная нижняя граница (\textbf{ELBO}).
        \item $KL(q || p)$ — Дивергенция Кульбака-Лейблера.
    \end{itemize}
    
    \textbf{Идея:} Вместо прямой максимизации $\ln p(X|\theta)$, мы итеративно максимизируем нижнюю границу $\mathcal{L}(q, \theta)$.
\end{frame}

% 7. Формулы ELBO и KL (НОВЫЙ СЛАЙД)
\begin{frame}{Формальное определение ELBO и KL}
    Подробный вид составляющих уравнения декомпозиции:

    \begin{block}{Вариационная нижняя граница (ELBO)}
        $$ \mathcal{L}(q, \theta) = \sum_{Z} q(Z) \ln \left( \frac{p(X, Z | \theta)}{q(Z)} \right) $$
    \end{block}

    \begin{block}{Дивергенция Кульбака-Лейблера (KL)}
        $$ KL(q || p) = - \sum_{Z} q(Z) \ln \left( \frac{p(Z | X, \theta)}{q(Z)} \right) $$
    \end{block}

    \vspace{0.2cm}
    \textbf{Свойство:} Согласно неравенству Гиббса, $KL(q || p) \ge 0$. Следовательно, $\mathcal{L}(q, \theta)$ действительно является \textbf{нижней оценкой} правдоподобия.
\end{frame}

% 8. Объяснение Гаммы (НОВЫЙ СЛАЙД)
\begin{frame}{Физический смысл переменной $\gamma$}
    Перед переходом к E-шагу введем ключевое понятие:

    \begin{block}{Ответственность (Responsibility) $\gamma_{nk}$}
        Это апостериорная вероятность того, что $n$-е наблюдение порождено $k$-й компонентой смеси:
        $$ \gamma_{nk} \equiv p(z_k = 1 | x_n) $$
    \end{block}

    \textbf{Интерпретация:}
    \begin{itemize}
        \item Она показывает степень нашей уверенности в том, к какому кластеру принадлежит точка.
        \item В алгоритме K-means принадлежность жесткая: $\gamma \in \{0, 1\}$.
        \item В GMM принадлежность мягкая: $\gamma \in [0, 1]$, при этом $\sum_{k} \gamma_{nk} = 1$.
    \end{itemize}
\end{frame}

% 9. E-шаг
\begin{frame}{E-шаг (Expectation)}
    \textbf{Задача:} Найти такое распределение $q(Z)$, которое минимизирует $KL$-дивергенцию (тем самым приравнивая ELBO к правдоподобию).
    
    Это достигается, когда $q(Z) = p(Z|X, \theta)$. Вычисляем «ответственности»:
    
    \begin{block}{Расчет $\gamma_{nk}$}
        $$ \gamma_{nk} = \frac{\pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_n | \mu_j, \Sigma_j)} $$
    \end{block}
    
    На этом шаге параметры $\theta$ фиксированы.
\end{frame}

% 10. M-шаг
\begin{frame}{M-шаг (Maximization)}
    \textbf{Задача:} Максимизировать ожидаемое правдоподобие (ELBO) по параметрам $\theta$ при фиксированных $\gamma_{nk}$.
    
    Формулы пересчета параметров:
    
    \begin{itemize}
        \item \textbf{Веса:} $\pi_k^{new} = \frac{N_k}{N}$, где $N_k = \sum_n \gamma_{nk}$.
        
        \item \textbf{Средние:} 
        $$ \mu_k^{new} = \frac{1}{N_k} \sum_{n=1}^{N} \gamma_{nk} x_n $$
        
        \item \textbf{Ковариации:}
        $$ \Sigma_k^{new} = \frac{1}{N_k} \sum_{n=1}^{N} \gamma_{nk} (x_n - \mu_k^{new})(x_n - \mu_k^{new})^T $$
    \end{itemize}
\end{frame}

% 11. Связь с K-means
\begin{frame}{Связь с алгоритмом K-means}
    K-means является частным (предельным) случаем GMM при наложении жестких ограничений:
    
    \begin{enumerate}
        \item \textbf{Фиксированная ковариация:} $\Sigma_k = \epsilon I$.
        \item \textbf{Фиксированные веса:} $\pi_k = 1/K$.
        \item \textbf{Предел $\epsilon \to 0$:} Мягкая вероятность $\gamma_{nk}$ превращается в жесткую метку.
    \end{enumerate}
    
    \vspace{0.2cm}
    \begin{center}
        % ИЗМЕНЕНИЕ ЗДЕСЬ:
        % Ограничиваем высоту (height) до 50% высоты слайда и сохраняем пропорции
        \includegraphics[height=0.5\textheight, keepaspectratio]{img_02.png} \\
        \footnotesize{Слева: K-means. Справа: GMM.}
    \end{center}
\end{frame}

% 12. Эксперимент: Сходимость
\begin{frame}{Эксперимент 1: Анализ сходимости}
    На графике представлена зависимость Log-Likelihood от номера итерации. Алгоритм монотонно увеличивает правдоподобие.
    
    \begin{center}
        \includegraphics[width=0.7\textwidth]{img_03.png}
    \end{center}
    
    \small{Реализована гибридная инициализация (K-means + EM) для избежания локальных минимумов.}
\end{frame}

% 13. Эксперимент: Анизотропия
\begin{frame}{Эксперимент 2: Анизотропные данные}
    Сравнение на вытянутых кластерах.
    
    \begin{columns}
        \column{0.4\textwidth}
        \begin{itemize}
            \item \textbf{K-means:} «Разрезает» кластеры, так как ищет сферы.
            \item \textbf{GMM:} Обучает полную матрицу $\Sigma$, подстраиваясь под наклон данных.
        \end{itemize}
        
        \column{0.6\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img_05.png}
    \end{columns}
\end{frame}

% 14. Эксперимент: Разная плотность
\begin{frame}{Эксперимент 3: Различная дисперсия}
    Ситуация: плотный кластер рядом с разреженным.
    
    \begin{center}
        \includegraphics[width=0.75\textwidth]{img_06.png}
    \end{center}
    
    GMM корректно определяет границы, учитывая разный «размер» (дисперсию) кластеров.
\end{frame}

% 15. Эксперимент: Сложная форма
\begin{frame}{Эксперимент 4: Невыпуклые данные}
    Аппроксимация датасета «Moons» ($K=10$).
    
    \begin{center}
        \includegraphics[width=0.6\textwidth]{img_07.png}
    \end{center}
    
    GMM может использоваться как универсальный аппроксиматор произвольной плотности вероятности.
\end{frame}

% 16. Эксперимент: Реальные данные
\begin{frame}{Эксперимент 5: Ирисы Фишера}
    Визуализация кластеризации реальных данных.
    
    \begin{center}
        \includegraphics[width=0.65\textwidth]{img_08.png}
    \end{center}
    
    Для пересекающихся классов (\textit{Versicolor} и \textit{Virginica}) алгоритм возвращает мягкие вероятности ($\gamma \approx 0.5$).
\end{frame}

% 17. Заключение
\begin{frame}{Заключение}
    \textbf{Результаты работы:}
    \begin{enumerate}
        \item Изучен математический аппарат EM-алгоритма.
        \item Разработана эффективная реализация на Python (Vectorization, Log-Sum-Exp trick).
        \item Экспериментально подтверждено преимущество GMM перед K-means на данных сложной структуры (анизотропия, разная плотность).
    \end{enumerate}
    
    \vspace{1cm}
    \centering
    \Large \textbf{Спасибо за внимание!}
\end{frame}

\end{document}