\documentclass[a4paper,14pt]{extarticle} % Используем 14 шрифт

%%% 1. НАСТРОЙКИ ЯЗЫКА И ШРИФТОВ %%%
\usepackage{cmap}                   
\usepackage[T2A]{fontenc}           
\usepackage[utf8]{inputenc}         
\usepackage[english,russian]{babel} 
\usepackage{mathptmx}               % Шрифт Times New Roman (похожий)

%%% 2. ГЕОМЕТРИЯ И ИНТЕРВАЛЫ %%%
% Поля: Левое 30мм, Правое 10мм, Верх/Низ 20мм
\usepackage[left=30mm,right=10mm,top=20mm,bottom=20mm]{geometry}
\usepackage{setspace}               
\onehalfspacing                     % Полуторный интервал
\usepackage{indentfirst}            % Красная строка в первом абзаце
\setlength{\parindent}{1.25cm}      % Размер абзацного отступа (стандарт)

%%% 3. МАТЕМАТИКА И ГРАФИКА %%%
\usepackage{amsmath, amsfonts, amssymb} 
\usepackage{graphicx}               
\graphicspath{{images/}}            
\usepackage[hidelinks]{hyperref}    % Ссылки без рамок

%%% 4. НАСТРОЙКИ ЗАГОЛОВКОВ (САМОЕ ВАЖНОЕ ДЛЯ БГУ) %%%
\usepackage{titlesec}

% Настройка ГЛАВ (Section)
% По центру, Жирным, Прописными, С новой строки номер и название
\titleformat{\section}[display]
    {\filcenter\bfseries\large}     % Стиль: по центру, жирный, чуть больше
    {ГЛАВА \thesection}             % Формат номера: ГЛАВА 1
    {0pt}                           % Отступ между номером и названием
    {\MakeUppercase}                % Все буквы заглавные автоматически

% Настройка РАЗДЕЛОВ (Subsection 1.1)
% С абзацного отступа, жирным
\titleformat{\subsection}
    {\bfseries\normalsize}
    {\hspace{1.25cm}\thesubsection} % Сдвиг номера на размер абзаца
    {1em}
    {}

% Настройка ПОДРАЗДЕЛОВ (Subsubsection 1.1.1)
\titleformat{\subsubsection}
    {\bfseries\normalsize}
    {\hspace{1.25cm}\thesubsubsection}
    {1em}
    {}

% Отступы вокруг заголовков
\titlespacing{\section}{0pt}{12pt}{12pt}
\titlespacing{\subsection}{0pt}{12pt}{6pt}

%%% 5. НАСТРОЙКИ ПОДПИСЕЙ (Рисунок 1.1 – Название) %%%
\usepackage[labelsep=endash]{caption} % Тире вместо двоеточия
\renewcommand{\figurename}{Рисунок}
\renewcommand{\tablename}{Таблица}

%%% 6. ОГЛАВЛЕНИЕ %%%
\usepackage{tocloft}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}} % Точки
\renewcommand{\cfttoctitlefont}{\hfill\bfseries\large\MakeUppercase} % Заголовок ОГЛАВЛЕНИЕ по центру
\renewcommand{\cftaftertoctitle}{\hfill}

%%% 7. НАСТРОЙКИ КОДА %%%
\usepackage{listings}
\usepackage{xcolor}
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,     % Шрифт кода чуть меньше основного
    keywordstyle=\color{blue}\bfseries,
    stringstyle=\color{red},
    commentstyle=\color{green!50!black},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    frame=single,
    tabsize=4,
    breaklines=true,
    breakatwhitespace=true,
    showstringspaces=false,
    inputencoding=utf8,
    extendedchars=true,
    literate={а}{{\selectfont\char224}}1 {б}{{\selectfont\char225}}1 {в}{{\selectfont\char226}}1 {г}{{\selectfont\char227}}1 {д}{{\selectfont\char228}}1 {е}{{\selectfont\char229}}1 {ё}{{\"e}}1 {ж}{{\selectfont\char230}}1 {з}{{\selectfont\char231}}1 {и}{{\selectfont\char232}}1 {й}{{\selectfont\char233}}1 {к}{{\selectfont\char234}}1 {л}{{\selectfont\char235}}1 {м}{{\selectfont\char236}}1 {н}{{\selectfont\char237}}1 {о}{{\selectfont\char238}}1 {п}{{\selectfont\char239}}1 {р}{{\selectfont\char240}}1 {с}{{\selectfont\char241}}1 {т}{{\selectfont\char242}}1 {у}{{\selectfont\char243}}1 {ф}{{\selectfont\char244}}1 {х}{{\selectfont\char245}}1 {ц}{{\selectfont\char246}}1 {ч}{{\selectfont\char247}}1 {ш}{{\selectfont\char248}}1 {щ}{{\selectfont\char249}}1 {ъ}{{\selectfont\char250}}1 {ы}{{\selectfont\char251}}1 {ь}{{\selectfont\char252}}1 {э}{{\selectfont\char253}}1 {ю}{{\selectfont\char254}}1 {я}{{\selectfont\char255}}1 {А}{{\selectfont\char192}}1 {Б}{{\selectfont\char193}}1 {В}{{\selectfont\char194}}1 {Г}{{\selectfont\char195}}1 {Д}{{\selectfont\char196}}1 {Е}{{\selectfont\char197}}1 {Ё}{{\"E}}1 {Ж}{{\selectfont\char198}}1 {З}{{\selectfont\char199}}1 {И}{{\selectfont\char200}}1 {Й}{{\selectfont\char201}}1 {К}{{\selectfont\char202}}1 {Л}{{\selectfont\char203}}1 {М}{{\selectfont\char204}}1 {Н}{{\selectfont\char205}}1 {О}{{\selectfont\char206}}1 {П}{{\selectfont\char207}}1 {Р}{{\selectfont\char208}}1 {С}{{\selectfont\char209}}1 {Т}{{\selectfont\char210}}1 {У}{{\selectfont\char211}}1 {Ф}{{\selectfont\char212}}1 {Х}{{\selectfont\char213}}1 {Ц}{{\selectfont\char214}}1 {Ч}{{\selectfont\char215}}1 {Ш}{{\selectfont\char216}}1 {Щ}{{\selectfont\char217}}1 {Ъ}{{\selectfont\char218}}1 {Ы}{{\selectfont\char219}}1 {Ь}{{\selectfont\char220}}1 {Э}{{\selectfont\char221}}1 {Ю}{{\selectfont\char222}}1 {Я}{{\selectfont\char223}}1
}

\begin{document}

%%% ТИТУЛЬНЫЙ ЛИСТ (Сделан по твоему скану) %%%
\begin{titlepage}
    \centering
    \footnotesize{БЕЛОРУССКИЙ ГОСУДАРСТВЕННЫЙ УНИВЕРСИТЕТ}\\
    \footnotesize{Факультет прикладной математики и информатики}\\
    \footnotesize{Кафедра дискретной математики и алгоритмики}\\
    
    \vspace{4cm}
    \textbf{\Large КУРСОВОЙ ПРОЕКТ}\\
    \vspace{0.5cm}
    \textbf{\large Тема: «Исследование и практическая реализация EM-алгоритма для задачи кластеризации на основе моделей смеси гауссовых распределений (Gaussian Mixture Models)»}
    
    \vspace{3cm}
    \begin{flushright}
        \begin{minipage}{0.5\textwidth}
            \raggedright
            Студент 3 курса 3 группы\\
            Санчук Сергей Александрович\\
            \vspace{1cm}
            Руководитель:\\
            Буславский Александр Андреевич
        \end{minipage}
    \end{flushright}
    
    \vfill
    Минск 2025
\end{titlepage}

%%% ОГЛАВЛЕНИЕ %%%
\tableofcontents
\newpage

%%% ВВЕДЕНИЕ %%%
\section*{ВВЕДЕНИЕ}
\addcontentsline{toc}{section}{ВВЕДЕНИЕ}

\textbf{Актуальность темы.} Задачи кластерного анализа занимают центральное место в современной теории машинного обучения и интеллектуального анализа данных (Data Mining). Необходимость выявления скрытой структуры в неразмеченных данных возникает в самых разных областях: от сегментации изображений и сжатия информации до биоинформатики и финансового скоринга.

Наиболее распространенным методом кластеризации является алгоритм K-means (метод $k$-средних). Несмотря на вычислительную эффективность, данный подход обладает существенными ограничениями: он предполагает сферическую форму кластеров и использует «жесткое» (hard) разбиение, при котором каждый объект однозначно приписывается одному кластеру. Однако реальные данные часто имеют сложную геометрическую структуру, могут обладать анизотропией (вытянутостью вдоль определенных направлений) и существенно перекрываться. В таких условиях использование метрических алгоритмов приводит к значительным ошибкам первого и второго рода.

Альтернативой выступает вероятностное моделирование, в частности использование моделей смеси гауссовых распределений (Gaussian Mixture Models, GMM). Данный подход позволяет аппроксимировать произвольную плотность распределения данных и реализует принцип «мягкой» (soft) кластеризации, возвращая вероятность принадлежности объекта к каждому из кластеров. Ключевой проблемой при использовании GMM является оценка параметров модели в условиях неполной информации (отсутствия меток классов). Стандартный метод максимального правдоподобия (MLE) в данном случае не имеет аналитического решения, что обуславливает необходимость применения итеративных методов оптимизации, основным из которых является EM-алгоритм (Expectation-Maximization).

Изучение математического аппарата EM-алгоритма и его программная реализация «с нуля» позволяют глубоко понять принципы работы с латентными переменными, проблемы численной стабильности и методы оптимизации функций правдоподобия, что является необходимым базисом для специалиста в области прикладной математики.

\vspace{0.5cm} % Небольшой отступ

\textbf{Объект исследования} --- вероятностные модели смеси распределений, используемые в задачах машинного обучения без учителя.

\textbf{Предмет исследования} --- EM-алгоритм как итерационный метод нахождения оценок максимального правдоподобия в вероятностных моделях со скрытыми переменными.

\textbf{Цель работы} --- теоретическое исследование математического обоснования EM-алгоритма и разработка его программной реализации для задачи кластеризации на основе смеси гауссовых распределений с ковариационной матрицей общего вида.

Для достижения поставленной цели необходимо решить следующие \textbf{задачи}:

\begin{enumerate}
    \item \textbf{Провести теоретический анализ} EM-алгоритма: изучить его вывод через вариационную нижнюю оценку (ELBO), обосновать шаги E (Expectation) и M (Maximization), а также рассмотреть условия сходимости последовательности оценок правдоподобия.
    \item \textbf{Исследовать свойства модели GMM:} проанализировать геометрическую интерпретацию параметров многомерного нормального распределения (вектор математического ожидания, ковариационная матрица) и их влияние на форму и ориентацию кластеров.
    \item \textbf{Выполнить сравнительный анализ} GMM и алгоритма K-means, выявив теоретические взаимосвязи (K-means как частный случай GMM) и различия в гибкости моделирования данных.
    \item \textbf{Разработать программную реализацию} алгоритма на языке Python с использованием библиотеки NumPy. Реализация должна поддерживать работу с ковариационными матрицами общего вида (Full Covariance) и включать механизмы обеспечения численной стабильности (Log-Sum-Exp trick).
    \item \textbf{Провести экспериментальное исследование} разработанного алгоритма на синтетических данных (включая анизотропные кластеры и данные различной плотности) и реальных наборах данных (Iris Fisher), оценив качество кластеризации с помощью метрики ARI (Adjusted Rand Index) и визуального анализа.
\end{enumerate}

\textbf{Методы исследования.} В работе использованы методы теории вероятностей, математической статистики (метод максимального правдоподобия, байесовский вывод), линейной алгебры (матричное дифференцирование, спектральное разложение) и методы оптимизации.

\textbf{Практическая значимость.} Разработанное программное обеспечение представляет собой гибкий инструмент для вероятностного анализа данных, способный, в отличие от стандартных метрических алгоритмов, корректно выделять кластеры сложной эллиптической формы и оценивать неопределенность принадлежности объектов к классам.

%%% ГЛАВА 1 %%%
\section{Теоретические основы моделирования смесей распределений}

В данной главе рассматривается математическая модель смеси гауссовых распределений (Gaussian Mixture Model, GMM), вводятся скрытые переменные для описания генеративной природы данных и формулируется задача оценки параметров методом максимального правдоподобия.

\subsection{Математическая модель GMM}

Модель смеси гауссовых распределений основывается на предположении, что наблюдаемые данные $X = \{x_1, \dots, x_N\}$, где $x_n \in \mathbb{R}^D$, порождены взвешенной суммой $K$ компонент, каждая из которых имеет нормальное распределение.

Функция плотности вероятности для произвольного вектора $x$ в модели GMM определяется следующим образом:
\begin{equation}
    p(x|\theta) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x | \mu_k, \Sigma_k),
\end{equation}
где:
\begin{itemize}
    \item $K$ --- количество компонент (кластеров) смеси;
    \item $\pi_k$ --- априорный вес (коэффициент смешивания) $k$-й компоненты, удовлетворяющий условиям нормировки:
    \begin{equation}
        0 \leq \pi_k \leq 1, \quad \sum_{k=1}^{K} \pi_k = 1;
    \end{equation}
    \item $\mathcal{N}(x | \mu_k, \Sigma_k)$ --- плотность $D$-мерного нормального распределения $k$-й компоненты;
    \item $\theta = \{\pi_1 \dots \pi_K, \mu_1 \dots \mu_K, \Sigma_1 \dots \Sigma_K\}$ --- полный набор параметров модели.
\end{itemize}

Многомерное нормальное распределение для $k$-й компоненты задается формулой:
\begin{equation}
    \mathcal{N}(x | \mu_k, \Sigma_k) = \frac{1}{\sqrt{(2\pi)^D |\Sigma_k|}} \exp \left( -\frac{1}{2} (x - \mu_k)^T \Sigma_k^{-1} (x - \mu_k) \right),
\end{equation}
где $\mu_k \in \mathbb{R}^D$ --- вектор математического ожидания (центр кластера), а $\Sigma_k \in \mathbb{R}^{D \times D}$ --- ковариационная матрица.

\subsubsection{Геометрическая интерпретация параметров}

Геометрия кластера в модели GMM полностью определяется параметрами $\mu_k$ и $\Sigma_k$. Линии уровня плотности вероятности (изолинии) многомерного гауссова распределения представляют собой эллипсоиды (в двумерном случае — эллипсы).

\begin{enumerate}
    \item \textbf{Вектор средних $\mu_k$} определяет положение центра симметрии эллипсоида в пространстве признаков.
    \item \textbf{Ковариационная матрица $\Sigma_k$} определяет форму, размер и ориентацию эллипсоида.
\end{enumerate}

В рамках данной работы рассматривается наиболее общий случай --- \textbf{полная ковариационная матрица (Full Covariance)}. Матрица $\Sigma_k$ является симметричной и положительно определенной. Ее свойства можно проанализировать через спектральное разложение (eigendecomposition):
\begin{equation}
    \Sigma_k = U_k \Lambda_k U_k^T = \sum_{j=1}^{D} \lambda_{kj} u_{kj} u_{kj}^T,
\end{equation}
где $\lambda_{kj}$ и $u_{kj}$ --- собственные числа и соответствующие им ортонормированные собственные векторы матрицы $\Sigma_k$.

\begin{itemize}
    \item \textbf{Собственные векторы $u_{kj}$} задают направления главных осей эллипсоида рассеяния. Наличие ненулевых недиагональных элементов в $\Sigma_k$ означает наличие корреляции между признаками, что геометрически выражается в \textbf{повороте} эллипсоида относительно осей координат.
    \item \textbf{Корни из собственных чисел $\sqrt{\lambda_{kj}}$} пропорциональны длине полуосей эллипсоида вдоль соответствующих направлений.
\end{itemize}

Использование полной ковариационной матрицы позволяет моделировать данные с анизотропной структурой, где кластеры могут быть вытянуты и наклонены под произвольным углом.

%%% КАРТИНКА 1 %%%
\begin{figure}[h]
    \centering
    % Убери знак процента ниже, когда положишь файл img_01.png в папку images
    \includegraphics[width=0.8\textwidth]{img_01.png} 
    \caption{Схематичное изображение GMM. Показаны несколько эллипсов разной ориентации, определяемые ковариационными матрицами.}
    \label{fig:gmm_scheme}
\end{figure}

\subsection{Байесовский подход к скрытым переменным}

Для формализации задачи кластеризации удобно рассматривать процесс генерации данных через введение \textbf{латентных (скрытых) переменных}.
Пусть $z$ --- дискретная случайная величина, представляющая собой $K$-мерный бинарный вектор, в котором только один элемент равен 1, а остальные — 0 (one-hot encoding). Если $z_k = 1$, это означает, что объект порожден $k$-й компонентой смеси.

Маргинальное распределение $z$ задается априорными весами:
\[ p(z_k = 1) = \pi_k. \]

Условное распределение наблюдаемого вектора $x$ при известном $z$ является нормальным:
\[ p(x | z_k = 1) = \mathcal{N}(x | \mu_k, \Sigma_k). \]

Совместное распределение наблюдаемых и скрытых переменных имеет вид $p(x, z) = p(x|z)p(z)$. Тогда маргинальное распределение $p(x)$, полученное суммированием по всем возможным состояниям $z$, возвращает нас к исходной формуле смеси:
\begin{equation}
    p(x) = \sum_{z} p(x|z)p(z) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x | \mu_k, \Sigma_k).
\end{equation}

Одной из ключевых задач кластеризации является определение апостериорной вероятности того, что наблюдаемый объект $x_n$ принадлежит кластеру $k$. Эту величину называют \textbf{«ответственностью» (responsibility)} $k$-й компоненты за $n$-й объект и обозначают $\gamma(z_{nk})$. Согласно теореме Байеса:
\begin{equation} \label{eq:responsibility}
    \gamma(z_{nk}) \equiv p(z_k = 1 | x_n) = \frac{p(z_k = 1) p(x_n | z_k = 1)}{\sum_{j=1}^{K} p(z_j = 1) p(x_n | z_j = 1)} = \frac{\pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_n | \mu_j, \Sigma_j)}.
\end{equation}

Величина $\gamma(z_{nk})$ принимает значения в интервале $[0, 1]$ и реализует принцип \textbf{«мягкой» кластеризации}: вместо жесткого присвоения метки алгоритм оценивает степень уверенности в принадлежности объекта к каждому из кластеров.

\subsection{Проблема оценки параметров}

Для настройки параметров модели $\theta$ используется метод максимального правдоподобия (Maximum Likelihood Estimation, MLE). Функция правдоподобия для выборки $X$ определяется как произведение плотностей вероятностей для всех независимых наблюдений:
\begin{equation}
    L(\theta | X) = \prod_{n=1}^{N} p(x_n | \theta) = \prod_{n=1}^{N} \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k).
\end{equation}

На практике максимизируют логарифм функции правдоподобия (Log-Likelihood), так как логарифм является монотонно возрастающей функцией и упрощает работу с экспоненциальным семейством распределений:
\begin{equation} \label{eq:loglike}
    \ln L(\theta | X) = \sum_{n=1}^{N} \ln \left( \sum_{k=1}^{K} \pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k) \right) \to \max_{\theta}.
\end{equation}

\textbf{Невозможность прямого аналитического решения}

Прямая максимизация данного выражения сталкивается с серьезными трудностями. Если попытаться найти экстремум, приравняв производные по параметрам к нулю, то наличие суммы под знаком логарифма ($\ln \sum \dots$) не позволяет получить аналитическое выражение для параметров в замкнутом виде.
Например, уравнение для $\mu_k$ будет зависеть от апостериорных вероятностей $\gamma(z_{nk})$, которые, в свою очередь, сложным образом зависят от всех параметров $\mu$, $\Sigma$ и $\pi$. Это приводит к системе нелинейных уравнений, не имеющей явного решения.

Данная проблема решается путем введения скрытых переменных $Z$ и перехода к итерационной процедуре оптимизации, известной как \textbf{EM-алгоритм}, математическое обоснование которого приводится в следующей главе.

%%% ГЛАВА 2 %%%
\section{Математическое обоснование EM-алгоритма}

В данной главе приводится формальный вывод алгоритма Expectation-Maximization (EM) как метода итеративной максимизации правдоподобия для вероятностных моделей со скрытыми переменными. Обосновывается введение вариационной нижней границы (ELBO) и детально рассматриваются шаги алгоритма для случая смеси гауссовых распределений (GMM).

\subsection{Вариационная нижняя граница (ELBO) и декомпозиция правдоподобия}

Основная сложность максимизации логарифма правдоподобия $\ln p(X|\theta)$ заключается в наличии скрытых переменных $Z$. Для решения этой проблемы преобразуем целевую функцию, введя произвольное распределение $q(Z)$ на множестве скрытых переменных.

Проведем формальный вывод соотношения, связывающего правдоподобие, вариационную нижнюю границу (ELBO) и дивергенцию Кульбака-Лейблера.

Запишем логарифм правдоподобия, умножив его на единицу в виде суммы вероятностей $q(Z)$ (условие нормировки $\sum_Z q(Z) = 1$):
\begin{equation}
    \ln p(X|\theta) = \ln p(X|\theta) \cdot \sum_{Z} q(Z) = \sum_{Z} q(Z) \ln p(X|\theta).
\end{equation}

Воспользуемся формулой условной вероятности $p(X, Z | \theta) = p(Z | X, \theta) p(X | \theta)$, откуда следует $p(X | \theta) = \frac{p(X, Z | \theta)}{p(Z | X, \theta)}$. Подставим это выражение под знак логарифма:
\begin{equation}
    \ln p(X|\theta) = \sum_{Z} q(Z) \ln \left( \frac{p(X, Z | \theta)}{p(Z | X, \theta)} \right).
\end{equation}

Умножим и разделим выражение под логарифмом на $q(Z)$:
\begin{equation}
    \ln p(X|\theta) = \sum_{Z} q(Z) \ln \left( \frac{p(X, Z | \theta)}{q(Z)} \cdot \frac{q(Z)}{p(Z | X, \theta)} \right).
\end{equation}

Используя свойство логарифма произведения ($\ln(ab) = \ln a + \ln b$), разобьем сумму на два слагаемых:
\begin{equation}
    \ln p(X|\theta) = \underbrace{\sum_{Z} q(Z) \ln \left( \frac{p(X, Z | \theta)}{q(Z)} \right)}_{\mathcal{L}(q, \theta)} + \underbrace{\sum_{Z} q(Z) \ln \left( \frac{q(Z)}{p(Z | X, \theta)} \right)}_{KL(q || p)}.
\end{equation}

Второе слагаемое с обратным знаком представляет собой определение дивергенции Кульбака-Лейблера (KL-дивергенции) между распределением $q(Z)$ и истинным апостериорным распределением $p(Z | X, \theta)$:
\begin{equation}
    KL(q || p) = - \sum_{Z} q(Z) \ln \left( \frac{p(Z | X, \theta)}{q(Z)} \right).
\end{equation}

Таким образом, мы получили фундаментальное тождество декомпозиции правдоподобия:
\begin{equation}
    \ln p(X|\theta) = \mathcal{L}(q, \theta) + KL(q || p).
\end{equation}

Где:
\begin{itemize}
    \item $\mathcal{L}(q, \theta)$ (\textbf{ELBO} --- Evidence Lower Bound) --- вариационная нижняя граница. Она зависит как от параметров модели $\theta$, так и от выбранного распределения $q(Z)$.
    \item $KL(q || p)$ --- мера расхождения между приближенным распределением $q(Z)$ и истинным апостериорным распределением скрытых переменных. Согласно неравенству Гиббса, $KL(q || p) \ge 0$, причем равенство достигается тогда и только тогда, когда $q(Z) = p(Z | X, \theta)$.
\end{itemize}

Из неотрицательности KL-дивергенции следует, что $\mathcal{L}(q, \theta)$ является нижней оценкой логарифма правдоподобия:
\begin{equation}
    \ln p(X|\theta) \ge \mathcal{L}(q, \theta).
\end{equation}

Этот вывод обосновывает стратегию EM-алгоритма: вместо сложной прямой максимизации $\ln p(X|\theta)$ мы итеративно максимизируем нижнюю границу $\mathcal{L}(q, \theta)$, последовательно приближая ее к истинному правдоподобию.

\subsection{Итерационный процесс оптимизации}

Алгоритм состоит из двух шагов, повторяющихся до сходимости: E-шаг (ожидание) и M-шаг (максимизация).

\subsubsection{E-шаг (Expectation)}

На этом шаге параметры модели $\theta$ фиксируются (используются значения $\theta^{(t)}$ с предыдущей итерации). Задача состоит в максимизации нижней границы $\mathcal{L}(q, \theta^{(t)})$ относительно распределения $q(Z)$.

Из уравнения декомпозиции видно, что $\ln p(X|\theta^{(t)})$ не зависит от $q(Z)$. Следовательно, максимизация ELBO эквивалентна минимизации $KL(q || p)$. Минимум достигается при:
\begin{equation}
    q^{(t+1)}(Z) = p(Z | X, \theta^{(t)}).
\end{equation}

Для модели GMM это означает вычисление апостериорных вероятностей («ответственностей») для каждого объекта $x_n$ и каждой компоненты $k$:
\begin{equation}
    \gamma_{nk} = \frac{\pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k)}{\sum_{j=1}^{K} \pi_j \mathcal{N}(x_n | \mu_j, \Sigma_j)}.
\end{equation}

\subsubsection{M-шаг (Maximization)}

На этом шаге фиксируется распределение $q(Z)$ (то есть значения $\gamma_{nk}$) и производится максимизация ожидания полного правдоподобия по параметрам $\theta$:
\begin{equation}
    Q(\theta, \theta^{(t)}) = E_Z [\ln p(X, Z | \theta)] = \sum_{n=1}^{N} \sum_{k=1}^{K} \gamma_{nk} \ln (\pi_k \mathcal{N}(x_n | \mu_k, \Sigma_k)) \to \max_{\theta}.
\end{equation}

Раскроем логарифм плотности нормального распределения:
\begin{equation}
    Q(\theta) = \sum_{n=1}^{N} \sum_{k=1}^{K} \gamma_{nk} \left( \ln \pi_k - \frac{1}{2}\ln |\Sigma_k| - \frac{1}{2}(x_n - \mu_k)^T \Sigma_k^{-1} (x_n - \mu_k) \right) + C.
\end{equation}

\textbf{1. Обновление средних $\mu_k$}

Продифференцируем $Q$ по $\mu_k$ и приравняем к нулю:
\begin{equation}
    \frac{\partial Q}{\partial \mu_k} = \sum_{n=1}^{N} \gamma_{nk} \Sigma_k^{-1} (x_n - \mu_k) = 0.
\end{equation}

Умножая на $\Sigma_k$, получаем:
\begin{equation} \label{eq:m_step_mu}
    \mu_k^{new} = \frac{\sum_{n=1}^{N} \gamma_{nk} x_n}{\sum_{n=1}^{N} \gamma_{nk}} = \frac{1}{N_k} \sum_{n=1}^{N} \gamma_{nk} x_n,
\end{equation}
где $N_k = \sum_{n=1}^{N} \gamma_{nk}$ --- эффективное число объектов в кластере $k$.

\textbf{2. Обновление ковариационной матрицы $\Sigma_k$}

Для вывода формулы обновления $\Sigma_k$ воспользуемся свойствами следа матрицы ($Tr$). Скалярное произведение в квадратичной форме можно записать через след:
\begin{equation}
    (x_n - \mu_k)^T \Sigma_k^{-1} (x_n - \mu_k) = Tr\left( (x_n - \mu_k)^T \Sigma_k^{-1} (x_n - \mu_k) \right) = Tr\left( \Sigma_k^{-1} (x_n - \mu_k)(x_n - \mu_k)^T \right).
\end{equation}

Тогда часть функции $Q$, зависящая от $\Sigma_k$, примет вид:
\begin{equation}
    Q_{\Sigma_k} = -\frac{1}{2} \sum_{n=1}^{N} \gamma_{nk} \left( \ln |\Sigma_k| + Tr\left( \Sigma_k^{-1} (x_n - \mu_k)(x_n - \mu_k)^T \right) \right).
\end{equation}

Введем матрицу разброса $S_k = \sum_{n=1}^{N} \gamma_{nk} (x_n - \mu_k)(x_n - \mu_k)^T$. Учитывая, что $\ln |\Sigma_k| = -\ln |\Sigma_k^{-1}|$, перепишем выражение относительно обратной матрицы $\Lambda_k = \Sigma_k^{-1}$:
\begin{equation}
    Q_{\Lambda_k} = \frac{1}{2} N_k \ln |\Lambda_k| - \frac{1}{2} Tr(\Lambda_k S_k).
\end{equation}

Воспользуемся формулами матричного дифференцирования:
\begin{equation}
    \frac{\partial \ln |\Lambda|}{\partial \Lambda} = \Lambda^{-T} = \Sigma^T = \Sigma \quad (\text{в силу симметрии}),
\end{equation}
\begin{equation}
    \frac{\partial Tr(\Lambda S)}{\partial \Lambda} = S^T = S.
\end{equation}

Дифференцируем $Q$ по $\Lambda_k$:
\begin{equation}
    \frac{\partial Q}{\partial \Lambda_k} = \frac{1}{2} N_k \Sigma_k - \frac{1}{2} S_k = 0.
\end{equation}

Отсюда получаем итоговую формулу для оценки полной ковариационной матрицы:
\begin{equation} \label{eq:m_step_sigma}
    \Sigma_k^{new} = \frac{S_k}{N_k} = \frac{\sum_{n=1}^{N} \gamma_{nk} (x_n - \mu_k^{new})(x_n - \mu_k^{new})^T}{N_k}.
\end{equation}

\textbf{3. Обновление весов $\pi_k$}

Для максимизации по $\pi_k$ необходимо учитывать ограничение $\sum \pi_k = 1$. Используем метод множителей Лагранжа:
\begin{equation}
    L(\pi, \lambda) = \sum_{k=1}^{K} N_k \ln \pi_k + \lambda \left( \sum_{k=1}^{K} \pi_k - 1 \right).
\end{equation}

Дифференцируя и приравнивая к нулю, получаем классический результат:
\begin{equation} \label{eq:m_step_pi}
    \pi_k^{new} = \frac{N_k}{N}.
\end{equation}

\subsection{Сходимость и статистические свойства}

Важнейшим свойством EM-алгоритма является монотонное неубывание правдоподобия на каждой итерации.
Пусть $\theta^{(t)}$ — параметры на шаге $t$, а $\theta^{(t+1)}$ — параметры, полученные после M-шага. Тогда справедливо неравенство:
\begin{equation}
    \ln p(X | \theta^{(t+1)}) \ge \ln p(X | \theta^{(t)}).
\end{equation}

\textbf{Доказательство (схема):}
\begin{enumerate}
    \item На E-шаге мы выбираем $q^{(t+1)}$ так, чтобы $KL(q || p) = 0$, следовательно $\ln p(X|\theta^{(t)}) = \mathcal{L}(q^{(t+1)}, \theta^{(t)})$.
    \item На M-шаге мы максимизируем $\mathcal{L}$ по $\theta$, поэтому $\mathcal{L}(q^{(t+1)}, \theta^{(t+1)}) \ge \mathcal{L}(q^{(t+1)}, \theta^{(t)})$.
    \item Для новых параметров $KL(q^{(t+1)} || p_{\theta^{(t+1)}}) \ge 0$.
    \item Суммируя, получаем: $\ln p(X|\theta^{(t+1)}) = \mathcal{L}(\dots) + KL(\dots) \ge \ln p(X|\theta^{(t)})$.
\end{enumerate}

Поскольку функция правдоподобия ограничена сверху (для корректно определенных моделей), последовательность значений сходится к локальному максимуму (или седловой точке).

Полученные оценки являются состоятельными, но смещенными (для конечных выборок). В рамках курсовой работы важно отметить, что EM-алгоритм не гарантирует нахождение глобального максимума, и результат сильно зависит от начальной инициализации параметров, что обуславливает необходимость использования стратегии мультистарта или инициализации методом K-means.

%%% ГЛАВА 3 %%%
\section{Сравнительный анализ алгоритмов кластеризации}

В задачах обучения без учителя выбор алгоритма критически зависит от структуры данных. В данной главе проводится теоретическое сравнение метода K-means (k-средних) и EM-алгоритма для GMM. Показано, что K-means является частным, предельным случаем GMM, накладывающим жесткие ограничения на модель данных.

\subsection{K-means как предельный случай GMM}

Алгоритм K-means минимизирует сумму квадратов внутрикластерных расстояний (Inertia):
\begin{equation}
    J = \sum_{n=1}^{N} \sum_{k=1}^{K} r_{nk} \|x_n - \mu_k\|^2,
\end{equation}
где $r_{nk} \in \{0, 1\}$ --- бинарный индикатор принадлежности.

Рассмотрим модель GMM с двумя ограничивающими допущениями:
\begin{enumerate}
    \item \textbf{Изотропность и равенство дисперсий.} Ковариационные матрицы всех компонент равны и пропорциональны единичной матрице с малым параметром $\epsilon$:
    \[ \Sigma_k = \epsilon I, \quad \forall k. \]
    \item \textbf{Равенство априорных вероятностей.} Веса компонент фиксированы: $\pi_k = \frac{1}{K}$.
\end{enumerate}

В этом случае плотность вероятности принимает вид:
\begin{equation}
    p(x | \mu_k, \Sigma_k) = \frac{1}{(2\pi\epsilon)^{D/2}} \exp \left( - \frac{1}{2\epsilon} \|x - \mu_k\|^2 \right).
\end{equation}

Рассмотрим E-шаг EM-алгоритма, вычисляющий апостериорную вероятность («ответственность»):
\begin{equation}
    \gamma_{nk} = \frac{\pi_k \exp \left( - \frac{1}{2\epsilon} \|x_n - \mu_k\|^2 \right)}{\sum_{j} \pi_j \exp \left( - \frac{1}{2\epsilon} \|x_n - \mu_j\|^2 \right)} = \frac{\exp \left( - \frac{1}{2\epsilon} \|x_n - \mu_k\|^2 \right)}{\sum_{j} \exp \left( - \frac{1}{2\epsilon} \|x_n - \mu_j\|^2 \right)}.
\end{equation}

Исследуем поведение $\gamma_{nk}$ при стремлении дисперсии к нулю ($\epsilon \to 0$).
В сумме экспонент доминирующим будет слагаемое, для которого показатель степени (квадрат евклидова расстояния $\|x_n - \mu_j\|^2$) минимален.
Пусть $k^* = \arg \min_j \|x_n - \mu_j\|^2$ --- индекс ближайшего к точке $x_n$ центроида. Тогда:
\begin{equation}
    \lim_{\epsilon \to 0} \gamma_{nk} = \begin{cases} 1, & \text{если } k = k^*, \\ 0, & \text{если } k \neq k^*. \end{cases}
\end{equation}

Таким образом, при исчезающе малой дисперсии «мягкое» вероятностное распределение вырождается в «жесткое» (hard) присвоение, эквивалентное шагу назначения меток в K-means. Соответственно, максимизация правдоподобия становится эквивалентной минимизации евклидовых расстояний.

\subsection{Геометрические и структурные различия}

Несмотря на родственную связь, в общем случае (при использовании полной ковариационной матрицы в GMM) алгоритмы демонстрируют принципиально разное поведение.

\begin{enumerate}
    \item \textbf{Гибкость формы кластеров (Сферичность vs Эллиптичность)}
    \begin{itemize}
        \item \textbf{K-means} неявно предполагает, что кластеры имеют сферическую форму. Границы разделения между кластерами всегда линейны (являются срединными перпендикулярами к отрезкам, соединяющим центроиды). Это приводит к ошибкам при работе с \textbf{анизотропными данными} (вытянутыми вдоль определенного направления). Алгоритм «разрезает» вытянутые структуры, пытаясь вписать их в сферы.
        \item \textbf{GMM (Full Covariance)} моделирует кластеры как эллипсоиды. Благодаря недиагональным элементам ковариационной матрицы $\Sigma_k$, модель способна учитывать корреляцию между признаками, поворачивая оси эллипсоида вдоль направления наибольшей дисперсии данных. Это позволяет корректно выделять наклонные и вытянутые кластеры.
    \end{itemize}

    %%% КАРТИНКА 2 %%%
    \begin{figure}[h]
        \centering
        % Убери знак процента ниже, когда положишь файл img_02.png в папку images
         \includegraphics[width=0.8\textwidth]{img_02.png}
        \caption{Схематичное сравнение границ разделения. Слева: K-means (линейные границы). Справа: GMM (квадратичные границы, эллипсы).}
        \label{fig:kmeans_vs_gmm}
    \end{figure}

    \item \textbf{Учет плотности и разброса данных}
    \begin{itemize}
        \item \textbf{K-means} чувствителен к различиям в диаметрах кластеров. Поскольку алгоритм использует только Евклидово расстояние, точки, находящиеся на периферии «широкого» (разреженного) кластера, могут быть ошибочно отнесены к соседнему «узкому» (плотному) кластеру, если его центр геометрически ближе.
        \item \textbf{GMM} оценивает собственные ковариационные матрицы для каждого кластера. Это позволяет модели корректно обрабатывать ситуацию, когда один кластер компактен (малая $|\Sigma|$), а другой имеет большой разброс (большая $|\Sigma|$). Расстояние Махаланобиса, неявно используемое в показателе экспоненты, нормирует удаленность точки на дисперсию соответствующего кластера.
    \end{itemize}

    \item \textbf{Характер неопределенности}
    
    K-means возвращает однозначный ответ, что может быть критично для граничных объектов. GMM возвращает вероятностный вектор. Это позволяет выявлять объекты со «смешанной» природой (например, $\gamma_{n1} \approx 0.49, \gamma_{n2} \approx 0.51$) и интерпретировать их как шумовые или переходные, что повышает интерпретируемость результатов анализа.
\end{enumerate}

\textbf{Вывод:} Использование GMM с полной матрицей ковариации теоретически обосновано для данных сложной геометрической структуры, тогда как область применимости K-means ограничена компактными, хорошо разделимыми группами сферической формы.

%%% ГЛАВА 4 %%%
\section{Программная реализация алгоритма}

В рамках курсового проекта была разработана собственная реализация EM-алгоритма для моделей смеси гауссовых распределений. Программный комплекс написан на языке Python 3.x с использованием библиотеки линейной алгебры NumPy. Реализация выполнена в парадигме объектно-ориентированного программирования (класс \texttt{GMM}), что обеспечивает модульность кода и удобство проведения экспериментов.

\subsection{Архитектура и векторные вычисления}

Ключевым требованием к реализации являлась эффективность вычислений при обработке больших массивов данных. В языке Python использование явных циклов (например, \texttt{for} по объектам выборки) приводит к существенному замедлению работы интерпретатора. Для решения этой проблемы была применена \textbf{векторизация вычислений}.

Входные данные представляются в виде матрицы $X$ размерности $(N \times D)$, где $N$ — число объектов, $D$ — размерность пространства признаков. Все параметры модели хранятся в виде многомерных массивов (тензоров):
\begin{itemize}
    \item Векторы средних \texttt{means}: матрица $(K \times D)$.
    \item Ковариационные матрицы \texttt{covs}: тензор $(K \times D \times D)$, что соответствует использованию полной ковариационной матрицы.
    \item Веса компонент \texttt{weights}: вектор длины $K$.
\end{itemize}

Особое внимание было уделено M-шагу, где происходит обновление параметров. Использование механизма транслирования (broadcasting) библиотеки NumPy позволило реализовать вычисление взвешенных ковариационных матриц без циклов по объектам, используя тензорные операции. Фрагмент реализации приведен в Листинге \ref{lst:update_covs}.

\begin{lstlisting}[caption={Векторизованное обновление ковариационных матриц}, label={lst:update_covs}]
def _update_covs(self, X: np.ndarray, gamma: np.ndarray) -> np.ndarray:
    """Обновление ковариационных матриц с использованием broadcasting."""
    N, D = X.shape
    N_k = np.sum(gamma, axis=0)
  
    # Создание тензора разностей (K, N, D)
    diff = X - self.means[:, None, :] 
  
    # Векторизованное вычисление взвешенной ковариации
    # gamma.T[:, :, None] имеет размерность (K, N, 1)
    numerator_part = gamma.T[:, :, None] * diff # (K, N, D)
    numerator_part = np.transpose(numerator_part, axes=(0, 2, 1)) # (K, D, N)
  
    # Матричное умножение (Batch matrix multiplication)
    numerator = numerator_part @ diff # Результат (K, D, D)
  
    covs = numerator / (N_k[:, None, None] + 1e-10)
  
    # Регуляризация (см. п. 4.2.2)
    covs[:, np.arange(D), np.arange(D)] += self.r

    return covs
\end{lstlisting}

\subsection{Обеспечение численной стабильности}

При прямой реализации формул EM-алгоритма возникают проблемы арифметического переполнения (overflow) или исчезновения порядков (underflow), связанные с ограниченной точностью чисел с плавающей точкой.

\subsubsection{Log-Sum-Exp Trick}

Функция плотности гауссова распределения содержит экспоненту. При высокой размерности $D$ значения вероятностей могут быть экстремально малыми, что интерпретируется компьютером как машинный ноль.

Для предотвращения этого все вычисления на E-шаге производятся в логарифмическом масштабе. Вместо вероятностей $p(x)$ вычисляются их логарифмы. Для вычисления логарифма суммы экспонент (необходимого для знаменателя в формуле Байеса) применяется тождество \textbf{Log-Sum-Exp}:
\begin{equation}
    \ln \sum_{i} \exp(x_i) = a + \ln \sum_{i} \exp(x_i - a), \quad \text{где } a = \max_i(x_i).
\end{equation}

Реализация E-шага с защитой от переполнения представлена в Листинге \ref{lst:e_step}.

\begin{lstlisting}[caption={Реализация E-шага (Log-Sum-Exp)}, label={lst:e_step}]
def _e_step(self, X: np.ndarray) -> Tuple[np.ndarray, float]:
    """E-шаг: Вычисление ответственности (Gamma) через Log-Sum-Exp."""
  
    # 1. Логарифм числителя: ln(pi_k) + ln(N(x|...))
    log_pdf = self._calc_log_pdf(X)
    log_weighted_pdf = log_pdf + np.log(self.weights + 1e-300) # (N, K)
  
    # 2. Log-Sum-Exp Trick для знаменателя
    # Находим максимум для каждого объекта (axis=1) для стабилизации
    max_log_weighted = np.max(log_weighted_pdf, axis=1, keepdims=True)
  
    # Вычисляем ln(sum(exp(x - max))) + max
    log_sum_exp = max_log_weighted + np.log(
        np.sum(np.exp(log_weighted_pdf - max_log_weighted), axis=1, keepdims=True)
    )
  
    # 3. Вычисление ln(gamma) = ln(числитель) - ln(знаменатель)
    log_gamma = log_weighted_pdf - log_sum_exp
    gamma = np.exp(log_gamma)
  
    # Возвращаем gamma и текущее значение Log-Likelihood
    return gamma, np.sum(log_sum_exp)
\end{lstlisting}

\subsubsection{Регуляризация ковариационной матрицы}

В процессе обучения ковариационная матрица $\Sigma_k$ может стать вырожденной (сингулярной), что делает невозможным её обращение. Для борьбы с этим применяется \textbf{Тихоновская регуляризация}: к главной диагонали матрицы ковариации на каждой итерации добавляется малая положительная константа $\epsilon$ (параметр \texttt{r} в коде).

В Листинге \ref{lst:update_covs} данная операция выполняется строкой:
\begin{lstlisting}[numbers=none, frame=none]
covs[:, np.arange(D), np.arange(D)] += self.r
\end{lstlisting}

Это гарантирует положительную определенность матрицы и геометрически предотвращает «схлопывание» эллипсоида кластера в плоскость.

\subsection{Стратегии инициализации}

Поскольку EM-алгоритм является локальным методом оптимизации, качество найденного решения критически зависит от начального приближения. В программе реализован гибридный подход: инициализация параметров с помощью алгоритма K-Means. Это позволяет начать оптимизацию GMM не со случайной точки, а с уже сформированных грубых кластеров.

Реализация инициализации приведена в Листинге \ref{lst:init}.

\begin{lstlisting}[caption={Инициализация через K-Means}, label={lst:init}]
def _init_with_kmeans(self, X: np.ndarray) -> None:
    """Инициализация параметров через K-Means."""
    N, D = X.shape
    # Используем стандартную реализацию для быстрого старта
    kmeans = KMeans(n_clusters=self.K, n_init=self.n_init, random_state=42)
    labels = kmeans.fit_predict(X)
  
    self.means = kmeans.cluster_centers_
    self.weights = np.zeros(self.K)
    self.covs = np.zeros((self.K, D, D))
  
    for k in range(self.K):
        X_k = X[labels == k]
        self.weights[k] = len(X_k) / N
      
        # Начальная оценка ковариации по результатам K-Means
        if len(X_k) > 1:
            self.covs[k] = np.cov(X_k, rowvar=False)
        else:
            self.covs[k] = np.eye(D)
      
        # Превентивная регуляризация
        self.covs[k] += np.eye(D) * self.r
\end{lstlisting}

Данный подход значительно ускоряет сходимость алгоритма и позволяет избегать субоптимальных локальных максимумов функции правдоподобия, что подтверждено экспериментально в Главе 5.

%%% ГЛАВА 5 %%%
\section{Экспериментальное исследование}

Целью экспериментальной части работы являлась верификация разработанной программной реализации EM-алгоритма, анализ устойчивости его сходимости, а также сравнение качества кластеризации с классическим методом K-means на наборах данных различной геометрической структуры.

Эксперименты проводились на синтетических и реальных данных. Для оценки качества кластеризации использовалась метрика ARI (Adjusted Rand Index), которая принимает значение 1.0 при полном совпадении с эталонной разметкой и значения около 0 при случайном разбиении.

\subsection{Анализ сходимости и устойчивости}

Первым этапом тестирования была проверка корректности работы оптимизационной процедуры. Эксперимент проводился на наборе данных \texttt{Isotropic Blobs} (сферические кластеры).
В ходе работы алгоритма фиксировалось значение логарифма правдоподобия (Log-Likelihood) на каждой итерации. График демонстрирует монотонный рост целевой функции и быстрый выход на плато, что подтверждает корректность реализации E-шага и M-шага.

%%% КАРТИНКА 3 %%%
\begin{figure}[h]
    \centering
     \includegraphics[width=0.8\textwidth]{img_03.png}
    \caption{Динамика максимизации функции правдоподобия (Log-Likelihood по итерациям).}
    \label{fig:loglike_conv}
\end{figure}

Также было проведено исследование влияния инициализации. При случайной инициализации параметров (\texttt{init\_random}) алгоритм в ряде запусков сходился к локальным экстремумам (кластеры «склеивались» или делились некорректно), показывая низкие значения Log-Likelihood (LL). Использование гибридной стратегии с инициализацией через K-means (\texttt{init\_kmeans}) обеспечило стабильную сходимость к глобальному максимуму во всех тестах.

%%% КАРТИНКА 4 %%%
\begin{figure}[h]
    \centering
     \includegraphics[width=0.9\textwidth]{img_04.png}
    \caption{Сравнение попыток запуска. Слева/Справа — разные исходы случайной инициализации, показывающие попадание в локальные минимумы.}
    \label{fig:init_compare}
\end{figure}

\subsection{Сравнительный анализ на синтетических данных}

Ключевой задачей было продемонстрировать преимущества использования \textbf{полной ковариационной матрицы} в модели GMM по сравнению с жесткими ограничениями алгоритма K-means.

\subsubsection{Анизотропные данные (Anisotropic Blobs)}

Набор данных содержит кластеры, сильно вытянутые вдоль диагонали и расположенные близко друг к другу. Это моделирует наличие сильной линейной корреляции между признаками.

\begin{itemize}
    \item \textbf{K-Means (ARI $\approx$ 0.88):} Алгоритм, основываясь на Евклидовом расстоянии, предполагает сферичность кластеров. В результате границы разделения перпендикулярны линии, соединяющей центры, что приводит к некорректному «разрезанию» вытянутых хвостов кластеров.
    \item \textbf{GMM (ARI = 1.0):} Благодаря обучению полной матрицы ковариации $\Sigma_k$, модель адаптировала форму эллипсов концентрации под структуру данных. Наклон главных осей эллипсов совпал с направлением разброса данных, что обеспечило идеальное разделение.
\end{itemize}

%%% КАРТИНКА 5 %%%
\begin{figure}[h]
    \centering
     \includegraphics[width=0.9\textwidth]{img_05.png}
    \caption{Сравнение на датасете Anisotropic. Слева: K-means (ошибки на границах). Справа: GMM (корректные наклонные эллипсы).}
    \label{fig:aniso}
\end{figure}

\subsubsection{Данные с различной дисперсией (Varied Variance)}

Эксперимент моделирует ситуацию, когда один кластер очень плотный (малая дисперсия), а соседний — разреженный (большая дисперсия).

\begin{itemize}
    \item \textbf{K-Means (ARI $\approx$ 0.79):} Граница проводится посередине между центроидами. Периферийные точки большого кластера, геометрически более близкие к центру малого кластера, ошибочно относятся к последнему.
    \item \textbf{GMM (ARI $\approx$ 0.95):} Алгоритм учитывает различную "ширину" распределений. Граница решений смещается в сторону более плотного кластера, что соответствует байесовскому правилу минимизации ошибки.
\end{itemize}

%%% КАРТИНКА 6 %%%
\begin{figure}[h]
    \centering
     \includegraphics[width=0.9\textwidth]{img_06.png}
    \caption{Сравнение на датасете Varied Variance. GMM корректно учитывает разный размер кластеров.}
    \label{fig:variance}
\end{figure}

\subsubsection{Невыпуклые данные (Moons)}

Датасет представляет собой два вложенных полумесяца. Это сложная задача для параметрических методов, так как данные не описываются одним нормальным распределением.

\begin{itemize}
    \item \textbf{K=2:} Оба алгоритма (K-means и GMM) не справились, проведя линейную/квадратичную границу.
    \item \textbf{K=10:} GMM продемонстрировал способность работать как универсальный аппроксиматор плотности. Сложная форма полумесяца была покрыта «цепочкой» из нескольких эллипсов. Это позволяет использовать GMM не только для кластеризации, но и для оценки плотности распределения сложной конфигурации.
\end{itemize}

%%% КАРТИНКА 7 %%%
\begin{figure}[h]
    \centering
     \includegraphics[width=0.8\textwidth]{img_07.png}
    \caption{Аппроксимация сложной формы датасета Moons с помощью GMM (K=10).}
    \label{fig:moons}
\end{figure}

\subsection{Апробация на реальных данных (Iris Fisher)}

Заключительный эксперимент проводился на классическом наборе данных ирисов Фишера (кластеризация по 4 признакам, визуализация по двум).

На графике видно, что класс \textit{Setosa} линейно отделим, и оба алгоритма справляются с ним успешно. Однако классы \textit{Versicolor} и \textit{Virginica} имеют зону естественного перекрытия.
В отличие от K-Means, который проводит жесткую границу, GMM построил мягкую модель: эллипсы ковариации накладываются друг на друга. Для спорных точек в области пересечения алгоритм вернул вероятности принадлежности $\gamma_{nk} \approx 0.5$, что объективно отражает природу данных и является важным преимуществом вероятностного подхода.

%%% КАРТИНКА 8 %%%
\begin{figure}[h]
    \centering
     \includegraphics[width=0.8\textwidth]{img_08.png}
    \caption{Результаты на Iris Dataset. Визуализация показывает мягкое пересечение эллипсов для перекрывающихся классов.}
    \label{fig:iris}
\end{figure}

\newpage
%%% ЗАКЛЮЧЕНИЕ %%%
\section*{ЗАКЛЮЧЕНИЕ}
\addcontentsline{toc}{section}{ЗАКЛЮЧЕНИЕ}

В ходе выполнения курсового проекта было проведено теоретическое исследование и практическая реализация алгоритма Expectation-Maximization для задачи кластеризации на основе смеси гауссовых распределений (GMM).

\textbf{Основные результаты работы:}

\begin{enumerate}
    \item \textbf{Теоретический базис.} Изучена математическая модель GMM и вывод формул EM-алгоритма через максимизацию вариационной нижней границы (ELBO). Показано, что классический алгоритм K-means является частным случаем GMM, возникающим при наложении жестких ограничений на ковариационные матрицы (сферичность) и вероятности (жесткая привязка).
    
    \item \textbf{Программная реализация.} Разработан программный модуль на языке Python (библиотека NumPy), реализующий GMM с \textbf{полной матрицей ковариации}. В реализации применены методы обеспечения численной стабильности:
    \begin{itemize}
        \item \textbf{Log-Sum-Exp Trick} для предотвращения арифметического переполнения при работе с малыми вероятностями.
        \item \textbf{Векторизация матричных операций} для повышения производительности.
        \item \textbf{Тихоновская регуляризация} для предотвращения сингулярности ковариационных матриц.
    \end{itemize}
    
    \item \textbf{Экспериментальные выводы.} Сравнительный анализ подтвердил теоретические преимущества GMM перед K-means:
    \begin{itemize}
        \item Модель с полной матрицей ковариации успешно кластеризует анизотропные данные, корректно определяя ориентацию кластеров (correlation aware).
        \item Вероятностная природа алгоритма позволяет корректно обрабатывать кластеры различной плотности и объема, а также возвращать меру уверенности в классификации («мягкая» кластеризация).
        \item Гибридная инициализация (K-means + EM) является предпочтительной стратегией, минимизирующей риск попадания в локальные экстремумы функции правдоподобия.
    \end{itemize}
\end{enumerate}

Таким образом, цель работы достигнута. Разработанный алгоритм демонстрирует высокую гибкость и может быть рекомендован для анализа данных со сложной геометрической структурой, где предположения метрических алгоритмов (таких как K-means) не выполняются.

\newpage
%%% СПИСОК ЛИТЕРАТУРЫ %%%

% 1. Добавляем раздел в Оглавление
\addcontentsline{toc}{section}{Список использованных источников}

% 2. Меняем заголовок на странице (чтобы было как в стандартах БГУ)
\renewcommand\refname{Список использованных источников}

\begin{thebibliography}{9}

    \bibitem{kharin} 
    Харин Ю. С., Зуев Н. М., Жук Е. Е. Теория вероятностей, математическая и прикладная статистика. — Минск: БГУ, 2011. — (Глава 14: Статистический анализ смесей распределений).

    \bibitem{bishop} 
    Bishop C. M. Pattern Recognition and Machine Learning. — Springer, 2006. — 738 p. (Chapter 9: Mixture Models and EM).
    
    \bibitem{murphy}
    Murphy K. P. Machine Learning: A Probabilistic Perspective. — MIT Press, 2012. — 1067 p.
    
    \bibitem{dempster}
    Dempster A. P., Laird N. M., Rubin D. B. Maximum Likelihood from Incomplete Data via the EM Algorithm // Journal of the Royal Statistical Society. Series B (Methodological). — 1977. — Vol. 39, No. 1. — P. 1–38.
    
    \bibitem{vanderplas}
    VanderPlas J. Python Data Science Handbook: Essential Tools for Working with Data. — O'Reilly Media, 2016. (Section: In Depth - Gaussian Mixture Models).
    
    \bibitem{sklearn}
    Документация библиотеки Scikit-learn: Gaussian Mixture Models [Электронный ресурс]. — URL: \url{https://scikit-learn.org/stable/modules/mixture.html} (дата обращения: 10.12.2025).

\end{thebibliography}

\end{document}